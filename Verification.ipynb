{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For verification, we will test our models on comments from the month of November 2015. In a similar manner as we scraped, we will take top-level comments from the top reddit threads of that month and predict scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1446350400..1448859600\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "mmddyy_range = ('11-01-2015', '11-30-2015')\n",
    "f_epoch = lambda x: int(time.mktime(time.strptime(x, '%m-%d-%Y')))\n",
    "time_range = str(f_epoch(mmddyy_range[0])) + '..' + str(f_epoch(mmddyy_range[1]))\n",
    "print time_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will thus scrape from the URL https://www.reddit.com/r/AskReddit/search?sort=top&q=timestamp%3A1446350400..1448859600&restrict_sr=on&syntax=cloudsearch, where we sort by \"top\" this time instead of \"comments\" -- we want threads that have been voted extensively upon. The result should be the top-level comments scraped from the highest-voted twenty-five threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "url = 'https://www.reddit.com/r/AskReddit/search?sort=top&q=timestamp%3A1446350400..1448859600&restrict_sr=on&syntax=cloudsearch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "import re\n",
    "\n",
    "data = BeautifulSoup(urllib2.urlopen(url).read(), 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3tdwy2', '3ucqxj', '3regbo', '3r2siv', '3secjr', '3ta5bz', '3si0bb', '3t1q82', '3tfpe2', '3ruhnj', '3r76bx', '3so18a', '3rbzln', '3tkemy', '3rih54', '3u2szy', '3u76qe', '3su6er', '3tid38', '3tqjr6', '3ur7jx', '3rg1rf', '3ualrk', '3ss8oz', '3rmbnh']\n"
     ]
    }
   ],
   "source": [
    "rows = data.findAll(\"a\", attrs = {'class': 'search-comments may-blank'}, limit=None)\n",
    "url_list = [str(r.attrs['href'])[:-17] + '.json' for r in rows]\n",
    "\n",
    "thread_ids = []\n",
    "for elt in url_list:\n",
    "    time.sleep(30)\n",
    "    json_text = urllib2.urlopen(elt).read()\n",
    "    thread_ids.append(re.search(r'\\\"id\\\": \\\"([a-zA-Z0-9_]+)\\\"', json_text).group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "r = praw.Reddit('Getting comments for CS109 Final Project')\n",
    "\n",
    "thread_d = {}\n",
    "for t_id in thread_ids:\n",
    "    time.sleep(10) # lots of HTTP request errors!\n",
    "    submission = r.get_submission(submission_id=t_id)\n",
    "    submission.replace_more_comments(limit=100, threshold=0)\n",
    "    all_comments = submission.comments\n",
    "    thread_d[t_id] = all_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "print len(thread_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36787\n"
     ]
    }
   ],
   "source": [
    "print sum([len(comments) for thread, comments in thread_d.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "36787\n"
     ]
    }
   ],
   "source": [
    "import cPickle\n",
    "cPickle.dump(thread_d, open('v_comments_dict.p', 'wb')) \n",
    "thread_d_load = cPickle.load(open('v_comments_dict.p', 'rb'))\n",
    "print len(thread_d_load)\n",
    "print sum([len(comments) for thread, comments in thread_d_load.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
