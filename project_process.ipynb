{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team (◕‿◕✿)'s Project Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scraping and cleaning our data\n",
    "\n",
    "We will scrape and clean our data with the following steps:\n",
    "\n",
    "1. Filter to AskReddit threads made in the last year.\n",
    "2. Get the thread IDs of the search results from step (1).\n",
    "3. Use praw to scrape parent comments from thread X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Filter using reddit search\n",
    "\n",
    "We want to get a decent sample size over a year -- we thus split a year's worth of data (11/01/2014 - 10/31/2015) into four parts of three months each. We then convert each of these time ranges into epochs, and format the range to use in URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "time_ranges = [('11-01-2014', '01-31-2015'), ('02-01-2015', '04-30-2015'), ('05-01-2015', '07-31-2015'), ('08-01-2015', '10-31-2015')] \n",
    "f_epoch = lambda x: int(time.mktime(time.strptime(x, '%m-%d-%Y')))\n",
    "time_ranges = [str(f_epoch(start)) + '..' + str(f_epoch(end)) for (start, end) in time_ranges]\n",
    "print time_ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each element in ```time_ranges```, we create a url to filter to AskReddit threads in that time range. The URL for the filtering is:\n",
    "\n",
    "https://www.reddit.com/r/AskReddit/search?sort=comments&q=timestamp%3A[time_range]&restrict_sr=on&syntax=cloudsearch\n",
    "\n",
    "Breaking down the above URL:\n",
    "* **/r/AskReddit**: filter to subreddit AskReddit\n",
    "* **sort=comments**: the search results will be sorted based on number of comments\n",
    "* **timestamp%3A1410739200..1411171200&**: restrict to 10/01/2014 00:00:00 to 10/01/2015 00:00:00, which are translated into epochs in the URL.\n",
    "* **syntax=cloudsearch**: reddit's search is infamously bad. One reason is that its regular search syntax, Lucene, doesn't allow for timestamp search. We thus choose CloudSearch as our syntax instead, which allows us to use features (like timestamp searches) that Lucene does not allow.\n",
    "\n",
    "reddit will return at most 1000 results for our search. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###1.2 Get thread IDs\n",
    "\n",
    "Now, we want to get all the search result URLs from our search. This sounds like a job for Beautiful Soup! First, using Developer Tools, we find our search contents:\n",
    "\n",
    "![Image of HTML searching](images/part1_1.png)\n",
    "![Image of HTML searching (2)](images/part1_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next parse the HTML using Beautiful Soup to get all the thread URLs for the search results. After getting the thread URLs, we append '.json' to the end of the URL, and using regex to extract out the thread ID from the resulting json. For each time range query (4 in total), we return 25 threads for a total of 100 threads, and thus the resulting ```thread_ids``` has 100 strings.\n",
    "\n",
    "(Note: the for-loop below can be written more nicely if reddit servers could handle more load! However, there are still HTTP error 429s after sleeping for 10 seconds. We circumvent this issue by checking how many URLs in ```url_list``` we manage to parse before hitting error 429, and continue appending from there.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "import re\n",
    "\n",
    "url_str1 = \"https://www.reddit.com/r/AskReddit/search?sort=comments&q=timestamp%3A\"\n",
    "url_str2 = \"&restrict_sr=on&syntax=cloudsearch\"\n",
    "url_list = []\n",
    "'''\n",
    "for tr in time_ranges[2:]:\n",
    "    time.sleep(30) # avoid HTTP request error\n",
    "    url = url_str1 + tr + url_str2\n",
    "    data = BeautifulSoup(urllib2.urlopen(url).read(), 'html.parser')\n",
    "    rows = data.findAll(\"a\", attrs = {'class': 'search-comments may-blank'}, limit=None)\n",
    "    url_list += [str(r.attrs['href'])[:-17] + '.json' for r in rows]\n",
    "print \"Done getting thread links!\"\n",
    "\n",
    "thread_ids = []\n",
    "for elt in url_list[18:]:\n",
    "    time.sleep(10)\n",
    "    json_text = urllib2.urlopen(elt).read()\n",
    "    thread_ids.append(re.search(r'\\\"id\\\": \\\"([a-zA-Z0-9_]+)\\\"', json_text).group(1))\n",
    "\n",
    "'''\n",
    "thread_ids = ['2rb0pa', '2qc6x6', '2pgxhq', '2nuals', '2ndc2r', '2qoe5v', '2nbuxe', '2noe2r', '2tv0hj', '2m64tg', \n",
    "              '2q8osf', '2rvvqw', '2sbi17', '2tedew', '2tx67q', '2typ2r', '2l9g71', '2podo2', '2q5zy0', '2ma5w6', \n",
    "              '2llwhe', \"2s5g55\", \"2lbckf\", \"2r7crn\", \"2tbexp\", \"348vlx\", \"2yhxa9\", \"34aqsn\", \"2usqzx\", \"2z8krp\", \n",
    "              \"31ifqb\", \"31q682\", \"335on6\", \"2xdqg2\", \"32i92t\", \"31nbca\", \"32g128\", \"349i6w\", \"2zqim0\", \"308dzi\", \n",
    "              \"3249ff\", \"2v39v2\", \"32xyr1\", \"2uibgu\", \"30wygs\", \"2x3qbr\", \"2veuez\", \"316ose\", \"32kf6l\", \"2xrzhg\", \n",
    "              \"3enigz\", \"37c2p3\", \"36959m\", \"38o5au\", \"39a7r8\", \"39fq7n\", \"3csgjk\", \"3862j5\", \"3bcd9y\", \"3f6k5e\", \n",
    "              \"351azq\", \"3b8brt\", \"3cii40\", \"3cbo1v\", \"3cfcxh\", \"36ih74\", \"395il3\", \"3amxh2\", \"37kq41\", \"3d4kpu\", \n",
    "              \"39twrl\", \"350629\", \"3dhkkp\", \"3a75dg\", \"3b29ew\", \"3g4blw\", \"3p03f5\", \"3qhw47\", \"3o0k0p\", \"3gljgr\", \n",
    "              \"3lf4rg\", \"3q5aw1\", \"3johsm\", \"3lnuyo\", \"3knx4k\", \"3prc2q\", \"3ld69q\", \"3l8ag4\", \"3hpxbx\", \"3fuyw1\", \n",
    "              \"3midk5\", \"3gaz4f\", \"3jm138\", \"3l5tvv\", \"3jqmuf\", \"3lx3rp\", \"3hfrb5\", \"3mowrl\", \"3iv9xy\", \"3hu28g\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Scrape comments using praw\n",
    "\n",
    "praw (Python Reddit API Wrapper), given the thread ID, will scrape all the parent comments from the thread. We will store the scraped comments in a dictionary: the keys are the thread ID, and the values are the list of parent comments in that thread. \n",
    "\n",
    "Unfortunately, the call ```replace_more_comments``` below takes up a lot of time, due to reddit's API request limits. It is possible to pass ```limit=None``` in ```replace_more_comments``` to get all the comments in the thread, but that often leads to HTTP error 429. We thus cap the number of requests at 100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "r = praw.Reddit('Getting comments for CS109 Final Project')\n",
    "\n",
    "#thread_d = {}\n",
    "for t_id in thread_ids[1:]:\n",
    "    time.sleep(10) # lots of HTTP request errors!\n",
    "    submission = r.get_submission(submission_id=t_id)\n",
    "    submission.replace_more_comments(limit=100, threshold=0)\n",
    "    all_comments = submission.comments\n",
    "    thread_d[t_id] = all_comments\n",
    "    print \"Got \" + str(len(all_comments)) + \" comments for \" + t_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phew! We don't want to go through that again, so let's pickle our dictionary into the file ```all_comments_dict.p```, so we have it ready-to-go for the next run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "cPickle.dump(thread_d, open('all_comments_dict.p', 'wb')) \n",
    "thread_d_load = cPickle.load(open('all_comments_dict.p', 'rb'))\n",
    "print thread_d_load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###1.4 Convert praw to JSON.\n",
    "\n",
    "(Note to self: on second thought, it might not be useful to convert to JSON)\n",
    "\n",
    "There are a lot of interesting attributes we can get from a praw object. For now, let's get: \n",
    "* the gold count\n",
    "* number of upvotes\n",
    "* time posted\n",
    "* comment body\n",
    "* author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'3sndsr': [<praw.objects.Comment object at 0x108509dd0>, <praw.objects.Comment object at 0x107db0a10>], '37kr5n': [<praw.objects.Comment object at 0x1082fedd0>]}\n"
     ]
    }
   ],
   "source": [
    "#from pprint import pprint\n",
    "#print all_comments\n",
    "#pprint (vars(all_comments[0]))\n",
    "#print all_comments[0].author"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
