{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team (◕‿◕✿)'s Project Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scraping and cleaning our data\n",
    "\n",
    "We will scrape and clean our data with the following steps:\n",
    "\n",
    "1. Filter to AskReddit threads made in the last year.\n",
    "2. Get the thread IDs of the search results from step (1).\n",
    "3. Use praw to scrape parent comments from thread X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Filter using reddit search\n",
    "\n",
    "We want to get a decent sample size over a year -- we thus split a year's worth of data (11/01/2014 - 10/31/2015) into four parts of three months each. We then convert each of these time ranges into epochs, and format the range to use in URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "time_ranges = [('11-01-2014', '01-31-2015'), ('02-01-2015', '04-30-2015'), ('05-01-2015', '07-31-2015'), ('08-01-2015', '10-31-2015')] \n",
    "f_epoch = lambda x: int(time.mktime(time.strptime(x, '%m-%d-%Y')))\n",
    "time_ranges = [str(f_epoch(start)) + '..' + str(f_epoch(end)) for (start, end) in time_ranges]\n",
    "print time_ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each element in ```time_ranges```, we create a url to filter to AskReddit threads in that time range. The URL for the filtering is:\n",
    "\n",
    "https://www.reddit.com/r/AskReddit/search?sort=comments&q=timestamp%3A[time_range]&restrict_sr=on&syntax=cloudsearch\n",
    "\n",
    "Breaking down the above URL:\n",
    "* **/r/AskReddit**: filter to subreddit AskReddit\n",
    "* **sort=comments**: the search results will be sorted based on number of comments\n",
    "* **timestamp%3A1410739200..1411171200&**: restrict to 10/01/2014 00:00:00 to 10/01/2015 00:00:00, which are translated into epochs in the URL.\n",
    "* **syntax=cloudsearch**: reddit's search is infamously bad. One reason is that its regular search syntax, Lucene, doesn't allow for timestamp search. We thus choose CloudSearch as our syntax instead, which allows us to use features (like timestamp searches) that Lucene does not allow.\n",
    "\n",
    "reddit will return at most 1000 results for our search. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###1.2 Get thread IDs\n",
    "\n",
    "Now, we want to get all the search result URLs from our search. This sounds like a job for Beautiful Soup! First, using Developer Tools, we find our search contents:\n",
    "\n",
    "![Image of HTML searching](images/part1_1.png)\n",
    "![Image of HTML searching (2)](images/part1_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next parse the HTML using Beautiful Soup to get all the thread URLs for the search results. After getting the thread URLs, we append '.json' to the end of the URL, and using regex to extract out the thread ID from the resulting json. For each time range query (4 in total), we return 25 threads for a total of 100 threads, and thus the resulting ```thread_ids``` has 100 strings.\n",
    "\n",
    "(Note: the for-loop below can be written more nicely if reddit servers could handle more load! However, there are still HTTP error 429s after sleeping for 10 seconds. We circumvent this issue by checking how many URLs in ```url_list``` we manage to parse before hitting error 429, and continue appending from there.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "import re\n",
    "\n",
    "url_str1 = \"https://www.reddit.com/r/AskReddit/search?sort=comments&q=timestamp%3A\"\n",
    "url_str2 = \"&restrict_sr=on&syntax=cloudsearch\"\n",
    "url_list = []\n",
    "for tr in time_ranges:\n",
    "    time.sleep(30) # avoid HTTP request error\n",
    "    url = url_str1 + tr + url_str2\n",
    "    data = BeautifulSoup(urllib2.urlopen(url).read(), 'html.parser')\n",
    "    rows = data.findAll(\"a\", attrs = {'class': 'search-comments may-blank'}, limit=None)\n",
    "    url_list += [str(r.attrs['href'])[:-17] + '.json' for r in rows]\n",
    "\n",
    "thread_ids = []\n",
    "for elt in url_list:\n",
    "    time.sleep(10)\n",
    "    json_text = urllib2.urlopen(elt).read()\n",
    "    thread_ids.append(re.search(r'\\\"id\\\": \\\"([a-zA-Z0-9_]+)\\\"', json_text).group(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Scrape comments using praw\n",
    "\n",
    "praw (Python Reddit API Wrapper), given the thread ID, will scrape all the parent comments from the thread. We will store the scraped comments in a dictionary: the keys are the thread ID, and the values are the list of parent comments in that thread. \n",
    "\n",
    "Unfortunately, the call ```replace_more_comments``` below takes up a lot of time, due to reddit's API request limits. It is possible to pass ```limit=None``` in ```replace_more_comments``` to get all the comments in the thread, but that often leads to HTTP error 429. We thus cap the number of requests at 100. The argument ```threshold``` checks how many replies the comment receives. We don't mind if our parent comment receives no replies, so we set ```threshold=0```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "164673\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "r = praw.Reddit('Getting comments for CS109 Final Project')\n",
    "\n",
    "thread_d = {}\n",
    "for t_id in thread_ids:\n",
    "    time.sleep(10) # lots of HTTP request errors!\n",
    "    submission = r.get_submission(submission_id=t_id)\n",
    "    submission.replace_more_comments(limit=100, threshold=0)\n",
    "    all_comments = submission.comments\n",
    "    thread_d[t_id] = all_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phew! We don't want to go through that again, so let's pickle our dictionary into the file ```all_comments_dict.p```, so we have it ready-to-go for the next run. Note that ```all_comments_dict.p``` is too big to upload to Github, so we had to zip the file ( ```all_comments_dict.p.zip```) in order to upload it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "import cPickle\n",
    "cPickle.dump(thread_d, open('all_comments_dict.p', 'wb')) \n",
    "thread_d_load = cPickle.load(open('all_comments_dict.p', 'rb'))\n",
    "print len(thread_d_load)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###What elements are interesting?\n",
    "\n",
    "We can look at the attributes of a PRAW comment and see what might be useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_has_fetched': True,\n",
      " '_has_fetched_replies': True,\n",
      " '_info_url': u'https://api.reddit.com/api/info/',\n",
      " '_replies': [<praw.objects.Comment object at 0x210966f50>],\n",
      " '_submission': <praw.objects.Submission object at 0x21092d6d0>,\n",
      " '_underscore_names': [u'replies'],\n",
      " '_uniq': None,\n",
      " 'approved_by': None,\n",
      " 'archived': True,\n",
      " 'author': Redditor(user_name='finndre'),\n",
      " 'author_flair_css_class': None,\n",
      " 'author_flair_text': None,\n",
      " 'banned_by': None,\n",
      " 'body': u'Private prisons with lockup quotas. States are contractually obligated to fill prison beds. wtf',\n",
      " 'body_html': u'<div class=\"md\"><p>Private prisons with lockup quotas. States are contractually obligated to fill prison beds. wtf</p>\\n</div>',\n",
      " 'controversiality': 0,\n",
      " 'created': 1420430099.0,\n",
      " 'created_utc': 1420401299.0,\n",
      " 'distinguished': None,\n",
      " 'downs': 0,\n",
      " 'edited': False,\n",
      " 'gilded': 0,\n",
      " 'id': u'cnecm22',\n",
      " 'json_dict': None,\n",
      " 'likes': None,\n",
      " 'link_id': u't3_2rb0pa',\n",
      " 'mod_reports': [],\n",
      " 'name': u't1_cnecm22',\n",
      " 'num_reports': None,\n",
      " 'parent_id': u't3_2rb0pa',\n",
      " 'reddit_session': <praw.Reddit object at 0x21092d790>,\n",
      " 'removal_reason': None,\n",
      " 'report_reasons': None,\n",
      " 'saved': False,\n",
      " 'score': 377,\n",
      " 'score_hidden': False,\n",
      " 'subreddit': Subreddit(subreddit_name='AskReddit'),\n",
      " 'subreddit_id': u't5_2qh1i',\n",
      " 'ups': 377,\n",
      " 'user_reports': []}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint (vars(thread_d_load['2rb0pa'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, the interesting attributes are:\n",
    "* the gold count\n",
    "* number of upvotes\n",
    "* time posted\n",
    "* comment body\n",
    "* author"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
